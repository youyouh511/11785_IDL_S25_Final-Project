{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a567984",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7badfb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import lib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import zipfile\n",
    "import os\n",
    "import os\n",
    "import zipfile\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "from typing import Any, Dict, List, Optional, Sequence, Tuple, Union\n",
    "from tqdm.auto import tqdm\n",
    "import seaborn as sns\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b61750",
   "metadata": {},
   "source": [
    "## Download and extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd34d6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_with_progress(url: str, destination: str) -> None:\n",
    "    \"\"\"Download a file from `url` to `destination` with a progress bar.\"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024  # 1 KiB\n",
    "\n",
    "    with open(destination, 'wb') as f, tqdm(\n",
    "        total=total_size,\n",
    "        unit='iB', unit_scale=True,\n",
    "        desc=f\"Downloading {os.path.basename(destination)}\",\n",
    "        ncols=100\n",
    "    ) as bar:\n",
    "        for chunk in response.iter_content(block_size):\n",
    "            f.write(chunk)\n",
    "            bar.update(len(chunk))\n",
    "\n",
    "\n",
    "def get_seasfire_datacube(version: str = '0.1') -> Optional[xr.Dataset]:\n",
    "    \"\"\"\n",
    "    Download and extract SeasFire cube for the given version.\n",
    "    Skips download if the ZIP exists, and skips extraction if the Zarr folder exists.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Determine URLs and filenames\n",
    "        if version == '0.1':\n",
    "            url = \"https://zenodo.org/records/6834585/files/SeasFireCube8daily.zip\"\n",
    "            zip_filename = \"SeasFireCube8daily.zip\"\n",
    "            extracted_folder = 'SeasFireCube8daily.zarr'\n",
    "        elif version == '0.4':\n",
    "            url = \"https://zenodo.org/records/13834057/files/seasfire_v0.4.zip\"\n",
    "            zip_filename = \"seasfire_v0.4.zip\"\n",
    "            extracted_folder = 'seasfire_v0.4.zarr'\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported version: {version}\")\n",
    "\n",
    "        # 1) Download step\n",
    "        if not os.path.exists(zip_filename):\n",
    "            print(f\"⬇Downloading data cube v{version}...\")\n",
    "            download_with_progress(url, zip_filename)\n",
    "        else:\n",
    "            print(f\"{zip_filename} already exists; skipping download.\")\n",
    "\n",
    "        # 2) Extraction step\n",
    "        if os.path.isdir(extracted_folder):\n",
    "            print(f\"{extracted_folder} already exists; skipping extraction.\")\n",
    "        else:\n",
    "            print(\"Extracting data cube...\")\n",
    "            with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "                members = zip_ref.namelist()\n",
    "                with tqdm(\n",
    "                    total=len(members), unit='file', desc='Extracting', ncols=100\n",
    "                ) as bar:\n",
    "                    for member in members:\n",
    "                        if not os.path.exists(member):\n",
    "                            zip_ref.extract(member)\n",
    "                        bar.update(1)\n",
    "\n",
    "        # 3) Verify and load\n",
    "        if not os.path.isdir(extracted_folder):\n",
    "            raise FileNotFoundError(f\"Extraction failed, '{extracted_folder}' not found.\")\n",
    "\n",
    "        # Decide if consolidated metadata is present\n",
    "        consolidated = os.path.exists(os.path.join(extracted_folder, '.zmetadata'))\n",
    "        # Load Zarr store via xarray\n",
    "        ds = xr.open_zarr(extracted_folder, consolidated=consolidated)\n",
    "        print(\"Dataset successfully loaded.\")\n",
    "        return ds\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while downloading or loading the data cube: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf38416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all download and extraction \n",
    "dataset = get_seasfire_datacube(version='0.4')\n",
    "# # Alternative if already downloaded and extracted\n",
    "# dataset = xr.open_dataset('seasfire_v0.4.zarr', engine='zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9513e7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset is loaded correctly\n",
    "dataset\n",
    "print(dataset['oci_ao'].shape)\n",
    "print(dataset['fcci_ba'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb4f710",
   "metadata": {},
   "source": [
    "## Data Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd10beca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_spatio_temporal_data(\n",
    "    data: xr.DataArray | xr.Dataset,\n",
    "    time_start: int,\n",
    "    time_length: int,\n",
    "    latitude: Optional[Union[int, Tuple[int,int]]] = None,\n",
    "    longitude: Optional[Union[int, Tuple[int,int]]] = None\n",
    ") -> xr.DataArray | xr.Dataset:\n",
    "    \"\"\"\n",
    "    Slice an xarray object in time (if present), latitude, and longitude.\n",
    "\n",
    "    Args:\n",
    "      data        : xarray DataArray or Dataset\n",
    "      time_start  : start index in 'time' coordinate (ignored if no 'time' dim)\n",
    "      time_length : number of consecutive timesteps\n",
    "      latitude    : None (all), int (single index), or (start, end)\n",
    "      longitude   : None (all), int (single index), or (start, end)\n",
    "\n",
    "    Returns:\n",
    "      The subset of `data` with the specified ranges.\n",
    "    \"\"\"\n",
    "    result = data\n",
    "\n",
    "    # Time slice (only if 'time' exists)\n",
    "    if 'time' in result.dims:\n",
    "        result = result.isel(time=slice(time_start, time_start + time_length))\n",
    "\n",
    "    # Helper to apply a 1D slice on a given dim\n",
    "    def _apply_slice(ds, dim, key):\n",
    "        if key is None:\n",
    "            return ds\n",
    "        if isinstance(key, int):\n",
    "            return ds.isel({dim: key})\n",
    "        if isinstance(key, (list, tuple)) and len(key) == 2:\n",
    "            return ds.isel({dim: slice(key[0], key[1])})\n",
    "        raise ValueError(f\"{dim!r} must be None, int, or (start,end)\")\n",
    "\n",
    "    # Latitude & Longitude\n",
    "    result = _apply_slice(result, 'latitude', latitude)\n",
    "    result = _apply_slice(result, 'longitude', longitude)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def plot_earth(\n",
    "    data: xr.DataArray | xr.Dataset,\n",
    "    time_start: int,\n",
    "    time_length: int,\n",
    "    latitude: None | int | tuple[int, int] = None,\n",
    "    longitude: None | int | tuple[int, int] = None,\n",
    "    col_wrap: int = 4\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot spatio-temporal slices of the data.\n",
    "\n",
    "    If time_length == 1, plots a single map; otherwise creates a faceted plot over time.\n",
    "    \"\"\"\n",
    "    subset = select_spatio_temporal_data(\n",
    "        data, time_start, time_length, latitude, longitude\n",
    "    )\n",
    "\n",
    "    if time_length == 1:\n",
    "        subset.plot()\n",
    "    else:\n",
    "        subset.plot(\n",
    "            x=\"longitude\",\n",
    "            y=\"latitude\",\n",
    "            col=\"time\",\n",
    "            col_wrap=col_wrap\n",
    "        )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def lat_lon_to_index(\n",
    "    coords: list[str],\n",
    "    dim: str,\n",
    "    size: int\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Convert human-readable lat/lon strings to array indices.\n",
    "\n",
    "    Args:\n",
    "      coords : list of strings like '25N', '45W'\n",
    "      dim    : 'latitude' or 'longitude'\n",
    "      size   : length of that dimension (e.g. 720 or 1440)\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    for val in coords:\n",
    "        deg = float(val[:-1])\n",
    "        dirc = val[-1].upper()\n",
    "        if dim == 'latitude':\n",
    "            if dirc == 'N':\n",
    "                idx = int((90 - deg) / 180 * size)\n",
    "            elif dirc == 'S':\n",
    "                idx = int((deg + 90) / 180 * size)\n",
    "            else:\n",
    "                raise ValueError(\"Latitude must end with 'N' or 'S'\")\n",
    "        elif dim == 'longitude':\n",
    "            if dirc == 'E':\n",
    "                idx = int((deg + 180) / 360 * size)\n",
    "            elif dirc == 'W':\n",
    "                idx = int((180 - deg) / 360 * size)\n",
    "            else:\n",
    "                raise ValueError(\"Longitude must end with 'E' or 'W'\")\n",
    "        else:\n",
    "            raise ValueError(\"dim must be 'latitude' or 'longitude'\")\n",
    "        indices.append(idx)\n",
    "    return sorted(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672264ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_idx = lat_lon_to_index(['25N','75N'], 'latitude', size=720)\n",
    "lon_idx = lat_lon_to_index(['15W','45E'], 'longitude', size=1440)\n",
    "plot_earth(dataset['gwis_ba'], 763, 5, latitude=tuple(lat_idx), longitude=tuple(lon_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaad3c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable summarization altogether\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56140a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot GFED region\n",
    "# grab the first time‐slice as a 2D numpy array\n",
    "data2d = dataset['gfed_region'].values\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(data2d,\n",
    "            cmap='tab20',        # or any matplotlib colormap you like\n",
    "            cbar_kws={'label': 'GFED Region'},\n",
    "            xticklabels=False,    # turn off to keep it fast\n",
    "            yticklabels=False)\n",
    "plt.title('GFED Region (t=0)')\n",
    "plt.xlabel('Longitude index')\n",
    "plt.ylabel('Latitude index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5394d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Build a mask (1 where biome==3, NaN elsewhere)\n",
    "mask3 = (dataset.biomes == 3).astype(float).where(dataset.biomes == 3)\n",
    "\n",
    "# 2) Define your bounds in human form…\n",
    "lat_bounds = ['25N', '75N']\n",
    "lon_bounds = ['15W', '45E']\n",
    "lat_idx = lat_lon_to_index(lat_bounds, 'latitude', size=dataset.dims['latitude'])\n",
    "lon_idx = lat_lon_to_index(lon_bounds, 'longitude', size=dataset.dims['longitude'])\n",
    "\n",
    "# 3) Convert to index‐pairs\n",
    "lat_idx = lat_lon_to_index(\n",
    "    coords=lat_bounds,\n",
    "    dim='latitude',\n",
    "    size=dataset.dims['latitude']\n",
    ")\n",
    "lon_idx = lat_lon_to_index(\n",
    "    coords=lon_bounds,\n",
    "    dim='longitude',\n",
    "    size=dataset.dims['longitude']\n",
    ")\n",
    "\n",
    "# 4) Extract the [t=0] subcube:\n",
    "subset = select_spatio_temporal_data(\n",
    "    data        = mask3,\n",
    "    time_start  = 0,\n",
    "    time_length = 1,\n",
    "    latitude    = (lat_idx[0], lat_idx[1]),\n",
    "    longitude   = (lon_idx[0], lon_idx[1])\n",
    ")\n",
    "\n",
    "subset = select_spatio_temporal_data(mask3, 763, 1, latitude=tuple(lat_idx), longitude=tuple(lon_idx))\n",
    "\n",
    "# 5) Plot:\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(\n",
    "    subset.values,\n",
    "    cmap='tab20',\n",
    "    cbar_kws={'label': 'GFED Region == 3'},\n",
    "    xticklabels=False,\n",
    "    yticklabels=False\n",
    ")\n",
    "plt.title(f\"Biome == 3 cropped to lat {lat_bounds} & lon {lon_bounds}\")\n",
    "plt.show()\n",
    "\n",
    "# # 6) Plot 2 ???:\n",
    "# print(\"Subset shape (t, lat, lon):\", subset.shape)\n",
    "# arr_1d = subset.values.ravel()\n",
    "\n",
    "# # 4) Plot histogram (log‐y)\n",
    "# plt.figure(figsize=(8, 4))\n",
    "# plt.hist(arr_1d, bins=50, log=True)\n",
    "# plt.title(\"Histogram of Burnt Area Values\\n(t=0–99, lat/lon slice)\")\n",
    "# plt.xlabel(\"Value\")\n",
    "# plt.ylabel(\"Frequency (log scale)\")\n",
    "# plt.grid(True, linestyle='--', alpha=0.5)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b815dfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21063d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Burn area distribution (sample: biomes==3, 2016)\n",
    "year = 2016\n",
    "gwis = dataset['gwis_ba'].sel(time=slice(f'{year}-01-01', f'{year}-12-31'))\n",
    "\n",
    "# Apply the spatial mask across all timesteps\n",
    "#    -> shape becomes (time, lat, lon) but only with biome==3 locations kept\n",
    "gwis3 = gwis.where(mask3)\n",
    "\n",
    "# Flatten to 1D and drop the NaNs\n",
    "flat = gwis3.values.ravel()\n",
    "flat = flat[~np.isnan(flat)]\n",
    "\n",
    "# Stats\n",
    "stats = {\n",
    "    'min':   np.min(flat),\n",
    "    '25%':   np.percentile(flat, 25),\n",
    "    'median':np.percentile(flat, 50),\n",
    "    'mean':  np.mean(flat),\n",
    "    '75%':   np.percentile(flat, 75),\n",
    "    'max':   np.max(flat),\n",
    "}\n",
    "\n",
    "\n",
    "# Plot histogram\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "sns.histplot(\n",
    "    flat,\n",
    "    bins=100,\n",
    "    stat='count',\n",
    "    log_scale=(False, True),\n",
    "    ax=ax,\n",
    "    color='lightgray'\n",
    ")\n",
    "\n",
    "# Overlay lines + annotate\n",
    "ylim = ax.get_ylim()[1]\n",
    "for name, val in stats.items():\n",
    "    ax.axvline(val, linestyle='--', label=f\"{name} = {val:.1f}\")\n",
    "    \n",
    "# Place a legend outside\n",
    "ax.legend(loc='upper right', title='Summary stats')\n",
    "\n",
    "# 5) Final touches\n",
    "ax.set_title(f\"gwis_ba Distribution in Biome==3 ({year})\")\n",
    "ax.set_xlabel(\"gwis_ba value\")\n",
    "ax.set_ylabel(\"Frequency (log scale)\")\n",
    "ax.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2364f5b",
   "metadata": {},
   "source": [
    "## Generate the train/val/test .json files under data folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eefb97a",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708d7d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_coords(\n",
    "    coords: np.ndarray,\n",
    "    shape: Tuple[int, int, int]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Convert (t, y, x) coords → flat indices.\"\"\"\n",
    "    return np.ravel_multi_index(coords.T, dims=shape, order='C')\n",
    "\n",
    "\n",
    "def unflatten_coords(\n",
    "    flat: np.ndarray,\n",
    "    shape: Tuple[int, int, int]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert flat indices → array of (t, y, x) coords.\n",
    "    \"\"\"\n",
    "    # NOTE: pass `shape` as a positional arg, not `dims=`\n",
    "    unraveled = np.unravel_index(flat, shape, order='C')\n",
    "    return np.column_stack(unraveled)\n",
    "\n",
    "\n",
    "def sample_new_coords(\n",
    "    shape: Tuple[int, int, int],\n",
    "    exclude: np.ndarray,\n",
    "    N: int,\n",
    "    mask2d: Optional[np.ndarray] = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Randomly sample N unique (t,y,x) coords in an array of shape `shape`,\n",
    "    excluding any in `exclude`, and (optionally) only where `mask2d==1`.\n",
    "    \"\"\"\n",
    "    n_t, n_y, n_x = shape\n",
    "    total = n_t * n_y * n_x\n",
    "\n",
    "    # 1) build valid flat pool\n",
    "    all_flat = np.arange(total)\n",
    "    if mask2d is not None:\n",
    "        # only allow flats whose spatial part is in mask\n",
    "        spatial_size = n_y * n_x\n",
    "        spatial_flat = all_flat % spatial_size\n",
    "        valid_spatial = mask2d.flatten(order='C').astype(bool)\n",
    "        valid_flat = all_flat[valid_spatial[spatial_flat]]\n",
    "    else:\n",
    "        valid_flat = all_flat\n",
    "\n",
    "    # 2) exclude already‐used\n",
    "    if exclude.size:\n",
    "        exclude_flat = flatten_coords(exclude, shape)\n",
    "        valid_flat = np.setdiff1d(valid_flat, exclude_flat, assume_unique=True)\n",
    "\n",
    "    # 3) ensure enough room\n",
    "    if N > valid_flat.size:\n",
    "        raise ValueError(f\"Cannot sample {N} coords; only {valid_flat.size} valid.\")\n",
    "\n",
    "    # 4) sample and unflatten\n",
    "    picked = np.random.choice(valid_flat, size=N, replace=False)\n",
    "    return unflatten_coords(picked, shape)\n",
    "\n",
    "\n",
    "def get_time_range(\n",
    "    mode: str,\n",
    "    time_lag: int,\n",
    "    step_size: int = 46\n",
    ") -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Return (start, length) for train/test/val splits.\n",
    "    Default step_size=46 (≈8-day windows per year).\n",
    "    \"\"\"\n",
    "    if mode == 'train':\n",
    "        return 0, step_size * 17\n",
    "    if mode == 'test':\n",
    "        return step_size * 17, step_size * 2\n",
    "    if mode == 'val':\n",
    "        return step_size * 19, step_size * 2\n",
    "    if mode == 'matrix':\n",
    "        return 0, step_size * 19\n",
    "    raise ValueError(\"mode must be 'train', 'test', 'val', 'matrix\")\n",
    "\n",
    "\n",
    "def load_variable_arrays(\n",
    "    ds: xr.Dataset,\n",
    "    var_names: Sequence[str],\n",
    "    time_start: int,\n",
    "    time_len: int,\n",
    "    lat_slice: Optional[Tuple[int,int]] = None,\n",
    "    lon_slice: Optional[Tuple[int,int]] = None\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extract variables from ds into NumPy arrays, slicing only\n",
    "    over dims that actually exist.\n",
    "\n",
    "    - time: slice(time_start, time_start+time_len) if present\n",
    "    - latitude: slice(lat_slice) if present and lat_slice is not None\n",
    "    - longitude: slice(lon_slice) if present and lon_slice is not None\n",
    "    \"\"\"\n",
    "    arrays: Dict[str, np.ndarray] = {}\n",
    "    t_sel = slice(time_start, time_start + time_len)\n",
    "\n",
    "    for v in var_names:\n",
    "        da = ds[v]\n",
    "\n",
    "        # 1) time slice if it exists\n",
    "        if \"time\" in da.dims:\n",
    "            da = da.isel(time=t_sel)\n",
    "\n",
    "        # 2) latitude slice if requested and exists\n",
    "        if lat_slice is not None and \"latitude\" in da.dims:\n",
    "            da = da.isel(latitude=slice(lat_slice[0], lat_slice[1]))\n",
    "\n",
    "        # 3) longitude slice if requested and exists\n",
    "        if lon_slice is not None and \"longitude\" in da.dims:\n",
    "            da = da.isel(longitude=slice(lon_slice[0], lon_slice[1]))\n",
    "\n",
    "        arrays[v] = da.values\n",
    "\n",
    "    return arrays\n",
    "\n",
    "\n",
    "def get_positive_negative_coords(\n",
    "    fire_arr: np.ndarray,\n",
    "    fire_threshold: float,\n",
    "    negative_ratio: int,\n",
    "    time_lag: int,\n",
    "    mask2d: Optional[np.ndarray] = None\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Return two arrays of coords:\n",
    "      - positives where fire_arr > threshold\n",
    "      - negatives sampled at random (negative_ratio × len(positives))\n",
    "    \"\"\"\n",
    "    shape = fire_arr.shape  # (T, Y, X)\n",
    "    flat = fire_arr.ravel()\n",
    "    pos_flat = np.nonzero(flat > fire_threshold)[0]\n",
    "    pos_coords = unflatten_coords(pos_flat, shape)\n",
    "\n",
    "    # sample negatives\n",
    "    N_neg = negative_ratio * pos_coords.shape[0]\n",
    "    neg_coords = sample_new_coords(\n",
    "        shape=shape,\n",
    "        exclude=pos_coords,\n",
    "        N=N_neg,\n",
    "        mask2d=mask2d\n",
    "    )\n",
    "\n",
    "    return pos_coords, neg_coords\n",
    "\n",
    "\n",
    "def build_instances(\n",
    "    pos_coords: np.ndarray,\n",
    "    neg_coords: np.ndarray,\n",
    "    label_vars: Dict[str, np.ndarray],\n",
    "    local_vars: Dict[str, np.ndarray],\n",
    "    oci_vars: Dict[str, np.ndarray],\n",
    "    time_lag: int,\n",
    "    initial_t: int\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Build the list of JSON objects from positive/negative coords and variable arrays.\n",
    "    \"\"\"\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for coords, label in ((pos_coords, 1), (neg_coords, 0)):\n",
    "        desc = 'pos' if label else 'neg'\n",
    "        for (t_rel, y, x) in tqdm(coords, desc=f\"Processing {desc}\", unit='pt'):\n",
    "            t_abs   = initial_t + time_lag + int(t_rel)\n",
    "            rel_idx = t_abs - initial_t\n",
    "            t0, t1  = rel_idx - time_lag, rel_idx\n",
    "\n",
    "            inst = {'local_variables': {}, 'ocis': {}, 'target': label}\n",
    "\n",
    "            # Spatial vars (time, y, x)\n",
    "            for var, arr in local_vars.items():\n",
    "                key = var.upper()\n",
    "                inst['local_variables'][key] = arr[t0:t1, y, x].tolist()\n",
    "\n",
    "            # OCI vars: either (time,y,x) or (time,)\n",
    "            for var, arr in oci_vars.items():\n",
    "                key = var.upper()\n",
    "                if arr.ndim == 3:\n",
    "                    # same indexing as local\n",
    "                    inst['ocis'][key] = arr[t0:t1, y, x].tolist()\n",
    "                elif arr.ndim == 1:\n",
    "                    # purely temporal → drop spatial\n",
    "                    inst['ocis'][key] = arr[t0:t1].tolist()\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected array shape for {var}: {arr.shape}\")\n",
    "\n",
    "            out.append(inst)\n",
    "\n",
    "    np.random.shuffle(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d841bbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(\n",
    "    ds: xr.Dataset,\n",
    "    mode: str = 'train',\n",
    "    latitude_bounds: List[str] = ['25N', '75N'],\n",
    "    longitude_bounds: List[str] = ['15W', '45E'],\n",
    "    negative_ratio: int = 5,\n",
    "    fire_threshold: float = 20000,\n",
    "    time_lag: int = 39,\n",
    "    fire_var: str = 'gwis_ba',\n",
    "    local_vars: Tuple[str,...] = (\"t2m_mean\", \"tp\", \"vpd\"),\n",
    "    oci_vars: Tuple[str,...] = (\"oci_nao\", \"oci_nina34_anom\", \"oci_ao\"),\n",
    "    mask2d: Optional[np.ndarray] = None,\n",
    "    out_path: Union[str, Path] = \"./data\"\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Build and save a dataset (NDJSON) of positive/negative fire examples.\n",
    "\n",
    "    Returns the path to the written `.json` file.\n",
    "    \"\"\"\n",
    "    # 0) check keys\n",
    "    required = (fire_var,) + local_vars + oci_vars\n",
    "    missing = [k for k in required if k not in ds]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing variables in dataset: {missing}\")\n",
    "\n",
    "    # 1) time ranges\n",
    "    initial_t, total_steps = get_time_range(mode, time_lag)\n",
    "    core_start = initial_t + time_lag\n",
    "    core_len   = total_steps - time_lag\n",
    "\n",
    "    # 2) spatial slices\n",
    "    lat_idx = lat_lon_to_index(latitude_bounds, 'latitude', ds.dims['latitude'])\n",
    "    lon_idx = lat_lon_to_index(longitude_bounds, 'longitude', ds.dims['longitude'])\n",
    "    lat_slice = (lat_idx[0], lat_idx[1])\n",
    "    lon_slice = (lon_idx[0], lon_idx[1])\n",
    "\n",
    "    # 3) load arrays\n",
    "    fire_arr = load_variable_arrays(ds, [fire_var],\n",
    "                                    core_start, core_len,\n",
    "                                    lat_slice, lon_slice)[fire_var]\n",
    "    local_arrs = load_variable_arrays(ds, local_vars,\n",
    "                                      initial_t, total_steps,\n",
    "                                      lat_slice, lon_slice)\n",
    "    oci_arrs   = load_variable_arrays(ds, oci_vars,\n",
    "                                      initial_t, total_steps,\n",
    "                                      lat_slice, lon_slice)\n",
    "\n",
    "    # 4) sample coords\n",
    "    pos_coords, neg_coords = get_positive_negative_coords(\n",
    "        fire_arr, fire_threshold, negative_ratio, time_lag, mask2d\n",
    "    )\n",
    "\n",
    "    # 5) build JSON instances\n",
    "    instances = build_instances(\n",
    "        pos_coords, neg_coords,\n",
    "        {fire_var: fire_arr},\n",
    "        local_arrs, oci_arrs,\n",
    "        time_lag, initial_t\n",
    "    )\n",
    "\n",
    "    # 6) write NDJSON\n",
    "    out_dir = Path(out_path)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_file = out_dir / f\"{mode}.json\"\n",
    "\n",
    "    with out_file.open('w', encoding='utf-8') as f:\n",
    "        for inst in instances:\n",
    "            f.write(json.dumps(inst, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Saved {len(instances)} examples to {out_file}\")\n",
    "    return out_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cd6d59",
   "metadata": {},
   "source": [
    "### Main builder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87520239",
   "metadata": {},
   "source": [
    "### JSON file generator\n",
    "\n",
    "Fire data is continuous variable. The fire_threshold can transform the continuous fire area variable to the binary status. Smaller fire_threshold, larger .json file created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c28a64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For real implementation, suggest the fire_threshold to be 1.\n",
    "fire_threshold = 1000\n",
    "modes = ['train', 'val', 'test', 'matrix']\n",
    "\n",
    "json_paths = {}\n",
    "for mode in modes:\n",
    "    print(f\"\\nBuilding {mode} partition\")\n",
    "    json_paths[mode] = build_dataset(\n",
    "        ds             = dataset,\n",
    "        mode           = mode,\n",
    "        fire_threshold = fire_threshold\n",
    "    )\n",
    "\n",
    "print(\"\\nGenerated files:\")\n",
    "for mode, path in json_paths.items():\n",
    "    print(f\"  {mode}: {path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
