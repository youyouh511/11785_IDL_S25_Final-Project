{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7badfb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import lib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import os\n",
    "import os\n",
    "import zipfile\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b61750",
   "metadata": {},
   "source": [
    "## Download the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd34d6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cube already downloaded.\n",
      "Extracting data cube...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting: 100%|███████████████████████████████████████████████| 927/927 [00:38<00:00, 24.18file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while downloading or loading the data cube: Extraction failed, SeasFireCube8daily_v0.4.zarr not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def download_with_progress(url, destination):\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024  # 1 Kilobyte\n",
    "\n",
    "    with open(destination, 'wb') as file, tqdm(\n",
    "        total=total_size, unit='iB', unit_scale=True, desc=destination, ncols=100, leave=True,\n",
    "        bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]\"\n",
    "    ) as bar:\n",
    "        for data in response.iter_content(block_size):\n",
    "            file.write(data)\n",
    "            bar.update(len(data))\n",
    "\n",
    "def get_version_1_of_seasfire_datacube(version='0.1'):\n",
    "    try:\n",
    "        # Download zipped cube\n",
    "\n",
    "        if version == '0.1':\n",
    "            url = \"https://zenodo.org/records/6834585/files/SeasFireCube8daily.zip\"\n",
    "            zip_filename = \"SeasFireCube8daily.zip\"\n",
    "        elif version == '0.4':\n",
    "            url = \"https://zenodo.org/records/13834057/files/seasfire_v0.4.zip\"\n",
    "            zip_filename = \"SeasFireCube8daily_v0.4.zip\"\n",
    "\n",
    "        if not os.path.exists(zip_filename):\n",
    "            print(\"Downloading data cube...\")\n",
    "            download_with_progress(url, zip_filename)\n",
    "        else:\n",
    "            print(\"Data cube already downloaded.\")\n",
    "\n",
    "        # Extract from zip file with progress bar\n",
    "        with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "            print(\"Extracting data cube...\")\n",
    "            total_files = len(zip_ref.namelist())\n",
    "            with tqdm(total=total_files, unit='file', desc='Extracting', ncols=100, leave=True,\n",
    "                      bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]\") as bar:\n",
    "                for file in zip_ref.namelist():\n",
    "                    zip_ref.extract(file)\n",
    "                    bar.update(1)\n",
    "\n",
    "        if version == '0.1':\n",
    "            extracted_folder = 'SeasFireCube8daily.zarr'\n",
    "        elif version == '0.4':\n",
    "            extracted_folder = 'SeasFireCube8daily_v0.4.zarr'\n",
    "            \n",
    "        if not os.path.exists(extracted_folder):\n",
    "            raise FileNotFoundError(f\"Extraction failed, {extracted_folder} not found.\")\n",
    "\n",
    "        # Load dataset\n",
    "        dataset = xr.open_zarr(extracted_folder)\n",
    "        print(\"Dataset successfully loaded.\")\n",
    "        return dataset\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while downloading or loading the data cube: {e}\")\n",
    "        return None\n",
    "\n",
    "dataset = get_version_1_of_seasfire_datacube(version='0.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baf38416",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb4f710",
   "metadata": {},
   "source": [
    "## Data processing functions \n",
    "\n",
    "(functions from the given ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd10beca",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Optional' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mselect_spatio_temporal_data\u001b[39m(\n\u001b[32m      2\u001b[39m     data: xr.DataArray | xr.Dataset,\n\u001b[32m      3\u001b[39m     time_start: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m      4\u001b[39m     time_length: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     latitude: \u001b[43mOptional\u001b[49m[Union[\u001b[38;5;28mint\u001b[39m, Tuple[\u001b[38;5;28mint\u001b[39m,\u001b[38;5;28mint\u001b[39m]]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m      6\u001b[39m     longitude: Optional[Union[\u001b[38;5;28mint\u001b[39m, Tuple[\u001b[38;5;28mint\u001b[39m,\u001b[38;5;28mint\u001b[39m]]] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m      7\u001b[39m ) -> xr.DataArray | xr.Dataset:\n\u001b[32m      8\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m    Slice an xarray object in time (if present), latitude, and longitude.\u001b[39;00m\n\u001b[32m     10\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m \u001b[33;03m      The subset of `data` with the specified ranges.\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     21\u001b[39m     result = data\n",
      "\u001b[31mNameError\u001b[39m: name 'Optional' is not defined"
     ]
    }
   ],
   "source": [
    "def select_spatio_temporal_data(\n",
    "    data: xr.DataArray | xr.Dataset,\n",
    "    time_start: int,\n",
    "    time_length: int,\n",
    "    latitude: Optional[Union[int, Tuple[int,int]]] = None,\n",
    "    longitude: Optional[Union[int, Tuple[int,int]]] = None\n",
    ") -> xr.DataArray | xr.Dataset:\n",
    "    \"\"\"\n",
    "    Slice an xarray object in time (if present), latitude, and longitude.\n",
    "\n",
    "    Args:\n",
    "      data        : xarray DataArray or Dataset\n",
    "      time_start  : start index in 'time' coordinate (ignored if no 'time' dim)\n",
    "      time_length : number of consecutive timesteps\n",
    "      latitude    : None (all), int (single index), or (start, end)\n",
    "      longitude   : None (all), int (single index), or (start, end)\n",
    "\n",
    "    Returns:\n",
    "      The subset of `data` with the specified ranges.\n",
    "    \"\"\"\n",
    "    result = data\n",
    "\n",
    "    # Time slice (only if 'time' exists)\n",
    "    if 'time' in result.dims:\n",
    "        result = result.isel(time=slice(time_start, time_start + time_length))\n",
    "\n",
    "    # Helper to apply a 1D slice on a given dim\n",
    "    def _apply_slice(ds, dim, key):\n",
    "        if key is None:\n",
    "            return ds\n",
    "        if isinstance(key, int):\n",
    "            return ds.isel({dim: key})\n",
    "        if isinstance(key, (list, tuple)) and len(key) == 2:\n",
    "            return ds.isel({dim: slice(key[0], key[1])})\n",
    "        raise ValueError(f\"{dim!r} must be None, int, or (start,end)\")\n",
    "\n",
    "    # Latitude & Longitude\n",
    "    result = _apply_slice(result, 'latitude', latitude)\n",
    "    result = _apply_slice(result, 'longitude', longitude)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def plot_earth(\n",
    "    data: xr.DataArray | xr.Dataset,\n",
    "    time_start: int,\n",
    "    time_length: int,\n",
    "    latitude: None | int | tuple[int, int] = None,\n",
    "    longitude: None | int | tuple[int, int] = None,\n",
    "    col_wrap: int = 4\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot spatio-temporal slices of the data.\n",
    "\n",
    "    If time_length == 1, plots a single map; otherwise creates a faceted plot over time.\n",
    "    \"\"\"\n",
    "    subset = select_spatio_temporal_data(\n",
    "        data, time_start, time_length, latitude, longitude\n",
    "    )\n",
    "\n",
    "    if time_length == 1:\n",
    "        subset.plot()\n",
    "    else:\n",
    "        subset.plot(\n",
    "            x=\"longitude\",\n",
    "            y=\"latitude\",\n",
    "            col=\"time\",\n",
    "            col_wrap=col_wrap\n",
    "        )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def lat_lon_to_index(\n",
    "    coords: list[str],\n",
    "    dim: str,\n",
    "    size: int\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Convert human-readable lat/lon strings to array indices.\n",
    "\n",
    "    Args:\n",
    "      coords : list of strings like '25N', '45W'\n",
    "      dim    : 'latitude' or 'longitude'\n",
    "      size   : length of that dimension (e.g. 720 or 1440)\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    for val in coords:\n",
    "        deg = float(val[:-1])\n",
    "        dirc = val[-1].upper()\n",
    "        if dim == 'latitude':\n",
    "            if dirc == 'N':\n",
    "                idx = int((90 - deg) / 180 * size)\n",
    "            elif dirc == 'S':\n",
    "                idx = int((deg + 90) / 180 * size)\n",
    "            else:\n",
    "                raise ValueError(\"Latitude must end with 'N' or 'S'\")\n",
    "        elif dim == 'longitude':\n",
    "            if dirc == 'E':\n",
    "                idx = int((deg + 180) / 360 * size)\n",
    "            elif dirc == 'W':\n",
    "                idx = int((180 - deg) / 360 * size)\n",
    "            else:\n",
    "                raise ValueError(\"Longitude must end with 'E' or 'W'\")\n",
    "        else:\n",
    "            raise ValueError(\"dim must be 'latitude' or 'longitude'\")\n",
    "        indices.append(idx)\n",
    "    return sorted(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2364f5b",
   "metadata": {},
   "source": [
    "## Generate the train/val/test .json files under data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "708d7d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_new_coords(shape, existing_coords, N):\n",
    "    \"\"\"\n",
    "    Randomly sample N distinct coordinates in an array of given shape,\n",
    "    excluding any in existing_coords.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    shape : tuple of ints\n",
    "        The overall grid shape, e.g. (100,100,100).\n",
    "    existing_coords : ndarray of shape (M, ndim)\n",
    "        Integer coordinates to exclude.\n",
    "    N : int\n",
    "        Number of new coordinates to sample.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    new_coords : ndarray of shape (N, ndim)\n",
    "        The newly sampled coordinates.\n",
    "    \"\"\"\n",
    "    shape = tuple(shape)\n",
    "    ndim = len(shape)\n",
    "\n",
    "    # 1) convert existing coords to flat indices\n",
    "    #    (coords.T gives a tuple of arrays for each dimension)\n",
    "    flat_existing = np.ravel_multi_index(existing_coords.T, shape)\n",
    "\n",
    "    # 2) total number of points\n",
    "    total = np.prod(shape)\n",
    "\n",
    "    # 3) build the set of available flat indices\n",
    "    #    (assume existing are unique -> faster)\n",
    "    all_flat = np.arange(total)\n",
    "    unused_flat = np.setdiff1d(all_flat, flat_existing, assume_unique=True)\n",
    "\n",
    "    if N > unused_flat.size:\n",
    "        raise ValueError(f\"Cannot sample {N} points; only {unused_flat.size} free.\")\n",
    "\n",
    "    # 4) choose N of them without replacement\n",
    "    chosen_flat = np.random.choice(unused_flat, size=N, replace=False)\n",
    "\n",
    "    # 5) map back to nd-coordinates\n",
    "    new_coords = np.column_stack(np.unravel_index(chosen_flat, shape))\n",
    "    return new_coords\n",
    "\n",
    "\n",
    "def build_dataset(type='train', latitude=['25N', '75N'], longitude=['15W', '45E'], NPR=5, fire_threshold=20000, time_lag=39, fire_var='BAs_GWIS', local_var=(\"t2m\", \"tp\", \"vpd_cf\"), ocis=(\"nao\", \"nina34_anom\")):\n",
    "\n",
    "    # 1) decide time ranges\n",
    "    if type == 'train':\n",
    "        initial_t, total_steps = 0, 46 * 17\n",
    "    elif type == 'test':\n",
    "        initial_t, total_steps = 46 * 17, 46 * 2\n",
    "    elif type == 'val':\n",
    "        initial_t, total_steps = 46 * 19, 46 * 2\n",
    "    else:\n",
    "        raise ValueError(\"Invalid type. Choose 'train','test','val'.\")\n",
    "\n",
    "    lat_idx = latitude2index(latitude)\n",
    "    lon_idx = longitude2index(longitude)\n",
    "\n",
    "    core_start = initial_t + time_lag\n",
    "    core_len   = total_steps - time_lag\n",
    "\n",
    "    # 2) load fire and predictors ONCE\n",
    "    fire_arr = select_spatio_temporal_data(dataset[fire_var], core_start, core_len, lat_idx, lon_idx).values\n",
    "\n",
    "    local_arrs = {\n",
    "        v: select_spatio_temporal_data(dataset[v], initial_t, total_steps, lat_idx, lon_idx).values\n",
    "        for v in local_var\n",
    "    }\n",
    "    oci_arrs = {\n",
    "        v: select_spatio_temporal_data(dataset[v], initial_t, total_steps, lat_idx, lon_idx).values\n",
    "        for v in ocis\n",
    "    }\n",
    "\n",
    "    # 3) find positives & sample negatives\n",
    "    flat = fire_arr.ravel()\n",
    "    pos_flat = np.nonzero(flat > fire_threshold)[0]\n",
    "    pos_coords = np.column_stack(np.unravel_index(pos_flat, fire_arr.shape))\n",
    "    neg_coords = sample_new_coords(fire_arr.shape, pos_coords, NPR * len(pos_coords))\n",
    "\n",
    "    # 4) build output\n",
    "    out = []\n",
    "    for coords, label, desc in ((pos_coords,1,\"pos\"), (neg_coords,0,\"neg\")):\n",
    "        pbar = tqdm(coords, desc=f\"Processing {desc}\", unit=\"pt\")\n",
    "        for t_rel, y, x in pbar:\n",
    "            t_abs   = core_start + int(t_rel)\n",
    "            rel_idx = t_abs - initial_t       # now in  [0 .. total_steps)\n",
    "            t0, t1 = rel_idx - time_lag, rel_idx\n",
    "\n",
    "            # Nested structure\n",
    "            inst = {\"local_variables\": {}, \"ocis\": {}, \"target\": label}\n",
    "\n",
    "            # fill local_variables with UPPERCASE keys\n",
    "            for v, arr in local_arrs.items():\n",
    "                key = v.upper()\n",
    "                inst[\"local_variables\"][key] = arr[t0:t1, y, x].tolist()\n",
    "\n",
    "            # fill ocis with UPPERCASE keys\n",
    "            for v, arr in oci_arrs.items():\n",
    "                key = v.upper()\n",
    "                inst[\"ocis\"][key] = arr[t0:t1, y, x].tolist()\n",
    "\n",
    "            out.append(inst)\n",
    "\n",
    "    import random\n",
    "    random.shuffle(out)\n",
    "\n",
    "    with open(f\"./data/{type}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for inst in out:\n",
    "            f.write(\n",
    "                json.dumps(inst, ensure_ascii=False, separators=(',', ':'))\n",
    "                + \"\\n\"\n",
    "            )\n",
    "    print(f\"Dataset saved to {type}.json (NDJSON, one object per line)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87520239",
   "metadata": {},
   "source": [
    "#### .json file generator.\n",
    "\n",
    "Fire data is continuous variable. The fire_threshold can transform the continuous fire area variable to the binary status. Smaller fire_threshold, larger .json file created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c28a64b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# For real implementation, suggest the fire_threshold to be 1.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mbuild_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfire_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m build_dataset(\u001b[38;5;28mtype\u001b[39m=\u001b[33m'\u001b[39m\u001b[33mval\u001b[39m\u001b[33m'\u001b[39m, fire_threshold=\u001b[32m10000\u001b[39m)\n\u001b[32m      5\u001b[39m build_dataset(\u001b[38;5;28mtype\u001b[39m=\u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m, fire_threshold=\u001b[32m10000\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mbuild_dataset\u001b[39m\u001b[34m(type, latitude, longitude, NPR, fire_threshold, time_lag, fire_var, local_var, ocis)\u001b[39m\n\u001b[32m     62\u001b[39m core_len   = total_steps - time_lag\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# 2) load fire and predictors ONCE\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m fire_arr = select_spatio_temporal_data(\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfire_var\u001b[49m\u001b[43m]\u001b[49m, core_start, core_len, lat_idx, lon_idx).values\n\u001b[32m     67\u001b[39m local_arrs = {\n\u001b[32m     68\u001b[39m     v: select_spatio_temporal_data(dataset[v], initial_t, total_steps, lat_idx, lon_idx).values\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m local_var\n\u001b[32m     70\u001b[39m }\n\u001b[32m     71\u001b[39m oci_arrs = {\n\u001b[32m     72\u001b[39m     v: select_spatio_temporal_data(dataset[v], initial_t, total_steps, lat_idx, lon_idx).values\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m ocis\n\u001b[32m     74\u001b[39m }\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# For real implementation, suggest the fire_threshold to be 1.\n",
    "\n",
    "build_dataset(type='train', fire_threshold=10000)\n",
    "build_dataset(type='val', fire_threshold=10000)\n",
    "build_dataset(type='test', fire_threshold=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f416a4",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb00b511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class JsonFireDataset(Dataset):\n",
    "    def __init__(self, json_path, local_keys=None, oci_keys=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            json_path (str): path to your NDJSON file (one JSON object per line).\n",
    "            local_keys (list of str): names of the local_variables channels, e.g. ['T2M','TP','VPD_CF'].\n",
    "                                      If None, inferred from the first sample.\n",
    "            oci_keys   (list of str): names of the ocis channels, e.g. ['NAO','NINA34_ANOM'].\n",
    "                                      If None, inferred from the first sample.\n",
    "        \"\"\"\n",
    "        # load all lines\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            self.records = [json.loads(line) for line in f]\n",
    "\n",
    "        # infer channel order if not given\n",
    "        first = self.records[0]\n",
    "        if local_keys is None:\n",
    "            local_keys = list(first['local_variables'].keys())\n",
    "        if oci_keys is None:\n",
    "            oci_keys = list(first['ocis'].keys())\n",
    "\n",
    "        self.local_keys = local_keys\n",
    "        self.oci_keys   = oci_keys\n",
    "        self.channel_keys = self.local_keys + self.oci_keys\n",
    "\n",
    "        # sanity-check that every record has the same length L\n",
    "        L = len(first['local_variables'][self.local_keys[0]])\n",
    "        for rec in self.records:\n",
    "            assert all(len(rec['local_variables'][k]) == L for k in self.local_keys), \"inconsistent L\"\n",
    "            assert all(len(rec['ocis'][k]) == L for k in self.oci_keys),       \"inconsistent L\"\n",
    "        self.L = L\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rec = self.records[idx]\n",
    "\n",
    "        # collect each channel sequence into a list\n",
    "        seqs = []\n",
    "        for k in self.local_keys:\n",
    "            seqs.append(rec['local_variables'][k])\n",
    "        for k in self.oci_keys:\n",
    "            seqs.append(rec['ocis'][k])\n",
    "\n",
    "        # stack → shape (channel, L)\n",
    "        x = torch.tensor(seqs, dtype=torch.float32)\n",
    "        y = torch.tensor(rec['target'], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "# ── USAGE ────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    # point these to your actual JSON file\n",
    "    train_ds = JsonFireDataset(\n",
    "        \"./data/train.json\",\n",
    "        local_keys=[\"T2M\",\"TP\",\"VPD_CF\"],\n",
    "        oci_keys=[\"NAO\",\"NINA34_ANOM\"],\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=2048,\n",
    "        shuffle=True,\n",
    "        num_workers=4,    # adjust to your machine\n",
    "        pin_memory=True,  # if you’re on GPU\n",
    "    )\n",
    "\n",
    "    # iterate\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        # batch_x: (32, 5, L)\n",
    "        # batch_y: (32,)\n",
    "        print(batch_x.shape, batch_y.shape)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dcd48d",
   "metadata": {},
   "source": [
    "## Others, delete in the final version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be54fb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = earth_graph(dataset['BAs_GWIS'], 0, 100, latitude2index(latitude), longitude2index(longitude), plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a27645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.values.shape)\n",
    "arr_1d = data.values.reshape(-1)  # or use arr.ravel()\n",
    "\n",
    "# Plot histogram\n",
    "plt.hist(arr_1d, bins=50, log=True)\n",
    "plt.title(\"Histogram of values\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
