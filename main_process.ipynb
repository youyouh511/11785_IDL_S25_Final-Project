{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ec945c3",
   "metadata": {},
   "source": [
    "# ðŸ”¥Causal Graph Neural Networks for Wildfire Danger PredictionðŸ”¥\n",
    "Re-implementation of original work by Zhao et al.(2024) (https://arxiv.org/abs/2403.08414)\n",
    "\n",
    "IDL S25 Group 23: Wenting Yue, Wenyu Liu, Youyou Huang (Group 23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e37dc0",
   "metadata": {},
   "source": [
    "## Retrieve files from github repository\n",
    "If `only notebook` is downloaded locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339ef5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/ubuntu/11785_IDL_S25_Final-Project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the current working directory\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "# repo = \"https://github.com/youyouh511/11785_IDL_S25_Final-Project.git\"\n",
    "# !git clone {repo}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8474fc73",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484a6211",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525a0d17",
   "metadata": {},
   "source": [
    "Environment setup\n",
    "```bash\n",
    "conda env create -f env.yml\n",
    "```\n",
    "\n",
    "Activate environment and check device\n",
    "```bash\n",
    "conda activate idl_final\n",
    "python -c \"import torch; print('CUDA available:', torch.cuda.is_available())\"\n",
    "nvidia-smi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f92f9e4",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861ab121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import (\n",
    "    JsonFireDataset,\n",
    ")\n",
    "from model import (\n",
    "    AdjacencyMatrix,\n",
    "    TemporalLSTM,\n",
    "    CausalGNN\n",
    ")\n",
    "from train import (\n",
    "    trainer\n",
    ")\n",
    "from utils import (\n",
    ")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import zipfile\n",
    "import torch\n",
    "import requests\n",
    "import xarray as xr\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3534b843",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01908115",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%wirtefile config.yaml\n",
    "\n",
    "###### Dataset\n",
    "data:\n",
    "    root                    : \"./input_data\"\n",
    "    train_partition         : \"train\"\n",
    "    val_partition           : \"val\"\n",
    "    test_partition          : \"test\"\n",
    "    train_json_path         : \"./input_data/train.json\"\n",
    "    val_json_path           : \"./input_data/val.json\"\n",
    "    test_json_path          : \"./input_data/test.json\"\n",
    "    matrix_json_path        : \"./input_data/matrix.json\"\n",
    "    subset                  : 1.0\n",
    "    batch_size              : 128\n",
    "    NUM_WORKERS             : 4\n",
    "\n",
    "    ###\n",
    "    fire_threshold          : 10\n",
    "\n",
    "model:\n",
    "    ### Adjacency matrix\n",
    "    local_var_lag           : 8\n",
    "    oci_var_lag             : 31\n",
    "    max_lag                 : 312\n",
    "    independence_test       : \"ParCorr\"\n",
    "    tau_max                 : 23\n",
    "    pc_alpha                : 0.05\n",
    "    mask_target             : True\n",
    "\n",
    "    ### Temporal LSTM\n",
    "    lstm_layer              : 1\n",
    "    hidden_dim              : 256\n",
    "\n",
    "    ### GNN\n",
    "    gnn_nodes               : 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577f018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb2ce69",
   "metadata": {},
   "source": [
    "# Data Retrieval & Pre-process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb6313c",
   "metadata": {},
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9bb875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_with_progress(url, destination):\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024  # 1 Kilobyte\n",
    "\n",
    "    with open(destination, 'wb') as file, tqdm(\n",
    "        total=total_size, unit='iB', unit_scale=True, desc=destination, ncols=100, leave=True,\n",
    "        bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]\"\n",
    "    ) as bar:\n",
    "        for data in response.iter_content(block_size):\n",
    "            file.write(data)\n",
    "            bar.update(len(data))\n",
    "\n",
    "\n",
    "def get_version_1_of_seasfire_datacube(version='0.1'):\n",
    "    try:\n",
    "        # Download zipped cube\n",
    "\n",
    "        if version == '0.1':\n",
    "            url = \"https://zenodo.org/records/6834585/files/SeasFireCube8daily.zip\"\n",
    "            zip_filename = \"SeasFireCube8daily.zip\"\n",
    "        elif version == '0.4':\n",
    "            url = \"https://zenodo.org/records/13834057/files/seasfire_v0.4.zip\"\n",
    "            zip_filename = \"SeasFireCube8daily_v0.4.zip\"\n",
    "\n",
    "        if not os.path.exists(zip_filename):\n",
    "            print(\"Downloading data cube...\")\n",
    "            download_with_progress(url, zip_filename)\n",
    "        else:\n",
    "            print(\"Data cube already downloaded.\")\n",
    "\n",
    "        # Extract from zip file with progress bar\n",
    "        with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "            print(\"Extracting data cube...\")\n",
    "            total_files = len(zip_ref.namelist())\n",
    "            with tqdm(total=total_files, unit='file', desc='Extracting', ncols=100, leave=True,\n",
    "                      bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]\") as bar:\n",
    "                for file in zip_ref.namelist():\n",
    "                    zip_ref.extract(file)\n",
    "                    bar.update(1)\n",
    "\n",
    "        if version == '0.1':\n",
    "            extracted_folder = 'SeasFireCube8daily.zarr'\n",
    "        elif version == '0.4':\n",
    "            extracted_folder = 'SeasFireCube8daily_v0.4.zarr'\n",
    "            \n",
    "        if not os.path.exists(extracted_folder):\n",
    "            raise FileNotFoundError(f\"Extraction failed, {extracted_folder} not found.\")\n",
    "\n",
    "        # Load dataset\n",
    "        dataset = xr.open_zarr(extracted_folder)\n",
    "        print(\"Dataset successfully loaded.\")\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while downloading or loading the data cube: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b2799e",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2615fcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_spatio_temporal_data(dataset, initial_timestep, timesteps, latitude, longitude):\n",
    "    '''\n",
    "    Desc\n",
    "    ----\n",
    "    Method to select a subset of the cube.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    dataset: xarray.Dataset\n",
    "        * the dataset from which we want the selected variable\n",
    "    initial_timestep: int\n",
    "        * value in the interval [0,965], where value represents the position of the date we want in the time array\n",
    "        * shows from what 8-day period we want to extract the data\n",
    "    timesteps: int\n",
    "        * value in the interval [1,966]\n",
    "        * shows for how many consecutive 8-days periods we want to extract the data\n",
    "    latitude: int\n",
    "        * vallue in the interval [0,719] where value represents the position of the latitude we want in the latitude array\n",
    "        * if we want all the latitudes, we set the value = -1\n",
    "    longitude: int\n",
    "        * vallue in the interval [0,14time_lag] where value represents the position of the longitude we want in the longitude array\n",
    "        * if we want all the longitudes, we set the value = -1\n",
    "    '''\n",
    "\n",
    "    if(latitude == -1 and longitude == -1):\n",
    "        return dataset.isel(time=slice(initial_timestep, initial_timestep+timesteps))\n",
    "    elif len(latitude)==2 and longitude == -1:\n",
    "        return dataset.isel(time=slice(initial_timestep, initial_timestep+timesteps), latitude=slice(latitude[0], latitude[1]))\n",
    "    elif len(longitude)==2 and latitude == -1:\n",
    "        return dataset.isel(time=slice(initial_timestep, initial_timestep+timesteps), longitude=slice(longitude[0], longitude[1]))\n",
    "    elif len(latitude)==2 and len(longitude)==2:\n",
    "        return dataset.isel(time=slice(initial_timestep, initial_timestep+timesteps), latitude=slice(latitude[0], latitude[1]), longitude=slice(longitude[0], longitude[1]))\n",
    "    else:\n",
    "        return dataset.isel(time=slice(initial_timestep, initial_timestep+timesteps), latitude=latitude, longitude=longitude)\n",
    "\n",
    "\n",
    "def earth_graph(dataset, initial_timestep, timesteps, latitude=-1, longitude=-1, col_wrap=1, plot=True):\n",
    "    '''\n",
    "    Desc\n",
    "    ----\n",
    "    Method to plot whole earth for specific variable for specific time interval.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    dataset: xarray.Dataset\n",
    "        * the dataset from which we want the selected variable\n",
    "    variable name: string\n",
    "        * name of the selected variable\n",
    "    initial_timestep: int\n",
    "        * value in the interval [0,965], where value represents the position of the date we want in the time array\n",
    "        * shows from what 8-day period we want to extract the data\n",
    "    timesteps: int\n",
    "        * value in the interval [1,966]\n",
    "        * shows for how many consecutive 8-days periods we want to extract the data\n",
    "    col_wrap: int\n",
    "        *how many graphs will be plotted i the same row\n",
    "    latitude: int\n",
    "        * value = -1, that is whole earth\n",
    "    longitude: int\n",
    "        * value = -1, that is whole earth\n",
    "    '''\n",
    "    if plot:\n",
    "        if (timesteps==1):\n",
    "\n",
    "            select_spatio_temporal_data(dataset,\n",
    "                                        initial_timestep,\n",
    "                                        timesteps,\n",
    "                                        latitude,\n",
    "                                        longitude).plot()\n",
    "        else:\n",
    "            select_spatio_temporal_data(dataset,\n",
    "                                        initial_timestep,\n",
    "                                        timesteps,\n",
    "                                        latitude,\n",
    "                                        longitude).plot(x=\"longitude\",\n",
    "                                                        y=\"latitude\",\n",
    "                                                        col=\"time\",\n",
    "                                                        col_wrap=col_wrap)\n",
    "    else:\n",
    "        if (timesteps==1):\n",
    "            return select_spatio_temporal_data(dataset,\n",
    "                                               initial_timestep,\n",
    "                                               timesteps,\n",
    "                                               latitude,\n",
    "                                               longitude)\n",
    "        else:\n",
    "            return select_spatio_temporal_data(dataset,\n",
    "                                               initial_timestep,\n",
    "                                               timesteps,\n",
    "                                               latitude,\n",
    "                                               longitude)\n",
    "\n",
    "latitude = ['25N', '75N']\n",
    "longitude = ['15W', '45E']\n",
    "\n",
    "Burnt_area = 'BAs_GWIS'\n",
    "\n",
    "def latitude_to_index(lat):\n",
    "    if lat[-1] == 'N':\n",
    "        return int((90 - float(lat[:-1])) / 180 * 720)\n",
    "    elif lat[-1] == 'S':    \n",
    "        return int((float(lat[:-1]) + 90) / 180 * 720)\n",
    "\n",
    "def longitude_to_index(lon):\n",
    "    if lon[-1] == 'W':\n",
    "        return int((180 - float(lon[:-1])) / 360 * 1440)\n",
    "    elif lon[-1] == 'E':    \n",
    "        return int((float(lon[:-1]) + 180) / 360 * 1440)\n",
    "\n",
    "def latitude2index(latitude):\n",
    "    return np.sort(np.array([latitude_to_index(lat) for lat in latitude])).tolist()\n",
    "def longitude2index(longitude):\n",
    "    return np.sort(np.array([longitude_to_index(lon) for lon in longitude])).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f6b50b",
   "metadata": {},
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727df45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_new_coords(shape, existing_coords, N):\n",
    "    \"\"\"\n",
    "    Randomly sample N distinct coordinates in an array of given shape,\n",
    "    excluding any in existing_coords.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    shape : tuple of ints\n",
    "        The overall grid shape, e.g. (100,100,100).\n",
    "    existing_coords : ndarray of shape (M, ndim)\n",
    "        Integer coordinates to exclude.\n",
    "    N : int\n",
    "        Number of new coordinates to sample.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    new_coords : ndarray of shape (N, ndim)\n",
    "        The newly sampled coordinates.\n",
    "    \"\"\"\n",
    "    shape = tuple(shape)\n",
    "    ndim = len(shape)\n",
    "\n",
    "    # 1) convert existing coords to flat indices\n",
    "    #    (coords.T gives a tuple of arrays for each dimension)\n",
    "    flat_existing = np.ravel_multi_index(existing_coords.T, shape)\n",
    "\n",
    "    # 2) total number of points\n",
    "    total = np.prod(shape)\n",
    "\n",
    "    # 3) build the set of available flat indices\n",
    "    #    (assume existing are unique -> faster)\n",
    "    all_flat = np.arange(total)\n",
    "    unused_flat = np.setdiff1d(all_flat, flat_existing, assume_unique=True)\n",
    "\n",
    "    if N > unused_flat.size:\n",
    "        raise ValueError(f\"Cannot sample {N} points; only {unused_flat.size} free.\")\n",
    "\n",
    "    # 4) choose N of them without replacement\n",
    "    chosen_flat = np.random.choice(unused_flat, size=N, replace=False)\n",
    "\n",
    "    # 5) map back to nd-coordinates\n",
    "    new_coords = np.column_stack(np.unravel_index(chosen_flat, shape))\n",
    "    return new_coords\n",
    "\n",
    "\n",
    "def build_dataset(dataset, type='train', latitude=['25N', '75N'], longitude=['15W', '45E'], NPR=5, fire_threshold=20000, time_lag=39, fire_var='BAs_GWIS', local_var=(\"t2m\", \"tp\", \"vpd_cf\"), ocis=(\"nao\", \"ao\", \"nina34_anom\")):\n",
    "\n",
    "    # 1) decide time ranges\n",
    "    if type == 'train':\n",
    "        initial_t, total_steps = 0, 46 * 17\n",
    "    elif type == 'test':\n",
    "        initial_t, total_steps = 46 * 17, 46 * 2\n",
    "    elif type == 'val':\n",
    "        initial_t, total_steps = 46 * 19, 46 * 2\n",
    "    elif type == 'matrix':\n",
    "        initial_t, total_steps = 0, 46 * 19\n",
    "    else:\n",
    "        raise ValueError(\"Invalid type. Choose 'train','test','val', 'matrix'.\")\n",
    "\n",
    "    lat_idx = latitude2index(latitude)\n",
    "    lon_idx = longitude2index(longitude)\n",
    "\n",
    "    core_start = initial_t + time_lag\n",
    "    core_len   = total_steps - time_lag\n",
    "\n",
    "    # 2) load fire and predictors ONCE\n",
    "    fire_arr = select_spatio_temporal_data(dataset[fire_var], core_start, core_len, lat_idx, lon_idx).values\n",
    "\n",
    "    local_arrs = {\n",
    "        v: select_spatio_temporal_data(dataset[v], initial_t, total_steps, lat_idx, lon_idx).values\n",
    "        for v in local_var\n",
    "    }\n",
    "    oci_arrs = {\n",
    "        v: select_spatio_temporal_data(dataset[v], initial_t, total_steps, lat_idx, lon_idx).values\n",
    "        for v in ocis\n",
    "    }\n",
    "\n",
    "    # 3) find positives & sample negatives\n",
    "    flat = fire_arr.ravel()\n",
    "    pos_flat = np.nonzero(flat > fire_threshold)[0]\n",
    "    pos_coords = np.column_stack(np.unravel_index(pos_flat, fire_arr.shape))\n",
    "    neg_coords = sample_new_coords(fire_arr.shape, pos_coords, NPR * len(pos_coords))\n",
    "\n",
    "    # 4) build output\n",
    "    out = []\n",
    "    for coords, label, desc in ((pos_coords,1,\"pos\"), (neg_coords,0,\"neg\")):\n",
    "        pbar = tqdm(coords, desc=f\"Processing {desc}\", unit=\"pt\")\n",
    "        for t_rel, y, x in pbar:\n",
    "            t_abs   = core_start + int(t_rel)\n",
    "            rel_idx = t_abs - initial_t       # now in  [0 .. total_steps)\n",
    "            t0, t1 = rel_idx - time_lag, rel_idx\n",
    "\n",
    "            # Nested structure\n",
    "            inst = {\"local_variables\": {}, \"ocis\": {}, \"target\": label}\n",
    "\n",
    "            # fill local_variables with UPPERCASE keys\n",
    "            for v, arr in local_arrs.items():\n",
    "                key = v.upper()\n",
    "                inst[\"local_variables\"][key] = arr[t0:t1, y, x].tolist()\n",
    "\n",
    "            # fill ocis with UPPERCASE keys\n",
    "            for v, arr in oci_arrs.items():\n",
    "                key = v.upper()\n",
    "                inst[\"ocis\"][key] = arr[t0:t1, y, x].tolist()\n",
    "\n",
    "            out.append(inst)\n",
    "\n",
    "    import random\n",
    "    random.shuffle(out)\n",
    "\n",
    "    with open(f\"./data/{type}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for inst in out:\n",
    "            f.write(\n",
    "                json.dumps(inst, ensure_ascii=False, separators=(',', ':'))\n",
    "                + \"\\n\"\n",
    "            )\n",
    "    print(f\"Dataset saved to {type}.json (NDJSON, one object per line)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff94012",
   "metadata": {},
   "source": [
    "# Run data retrieval & filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb400690",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_version_1_of_seasfire_datacube(version='0.4')\n",
    "earth_graph(dataset['FCCI_BA'], 763, 1, latitude2index(latitude), longitude2index(longitude))\n",
    "\n",
    "# Export json files\n",
    "# For real implementation, suggest the fire_threshold to be 1?\n",
    "thredshold = config['data']['fire_threshold']\n",
    "build_dataset(type='matrix', fire_threshold=thredshold)\n",
    "build_dataset(type='train', fire_threshold=thredshold)\n",
    "build_dataset(type='val', fire_threshold=thredshold)\n",
    "build_dataset(type='test', fire_threshold=thredshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572cbc51",
   "metadata": {},
   "source": [
    "# Adjacency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61c6a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_builder = AdjacencyMatrix(config['data']['matrix_json_path'], )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b39eef",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ab8390",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = JsonFireDataset(\n",
    "    json_path   = \"./data/train.json\",\n",
    "    local_keys  = [\"T2M\",\"TP\",\"VPD_CF\"],\n",
    "    oci_keys    = [\"NAO\", \"AO\", \"NINA34_ANOM\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idl_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
